{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InscribeDeeper/bert_utils/blob/master/qa_bert_pingenbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "3SsX3j3sfBKk"
      },
      "source": [
        "<h1>Table of Contents: training with sub dataset<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Baseline-3:-BioBERT-Pretrained---CNN-only\" data-toc-modified-id=\"Baseline-3:-BioBERT-Pretrained---CNN-only-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Baseline 3: BioBERT Pretrained - CNN only</a></span></li><li><span><a href=\"#1.-Setup\" data-toc-modified-id=\"1.-Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>1. Setup</a></span></li><li><span><a href=\"#2.-Parse-data\" data-toc-modified-id=\"2.-Parse-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>2. Parse data</a></span></li><li><span><a href=\"#3.-Tokenization-&amp;-Input-Formatting\" data-toc-modified-id=\"3.-Tokenization-&amp;-Input-Formatting-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>3. Tokenization &amp; Input Formatting</a></span></li><li><span><a href=\"#4.-Define-model\" data-toc-modified-id=\"4.-Define-model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>4. Define model</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#6.1.-Evalution-Function\" data-toc-modified-id=\"6.1.-Evalution-Function-5.0.1\"><span class=\"toc-item-num\">5.0.1&nbsp;&nbsp;</span>6.1. Evalution Function</a></span></li></ul></li><li><span><a href=\"#6.3.-4-fold-cross-validation;-one-vs-the-rest\" data-toc-modified-id=\"6.3.-4-fold-cross-validation;-one-vs-the-rest-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>6.3. 4-fold cross validation; one-vs-the-rest</a></span></li></ul></li><li><span><a href=\"#7.-Train-a-model-with-all-data-for-prediction\" data-toc-modified-id=\"7.-Train-a-model-with-all-data-for-prediction-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>7. Train a model with all data for prediction</a></span></li><li><span><a href=\"#Predict-sentences\" data-toc-modified-id=\"Predict-sentences-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Predict sentences</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKOTlwcmxmej"
      },
      "source": [
        "# Baseline 3: BioBERT Pretrained - CNN only\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w8elk83nAFKM"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bmmaqGLR1xx",
        "outputId": "eaa2cfd2-dc3c-4e79-d7a3-97c5dc1eb2bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WqHIo4Bzmcef"
      },
      "outputs": [],
      "source": [
        "#pip install --target=$package_path torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_ZDhicpHkV"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LealyJc7ZC8b",
        "outputId": "a1a6ab77-d59d-4f50-f840-ea75a1042048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "# nb_path = '/content/rl'\n",
        "# #os.symlink('/content/drive/MyDrive/Colab_Notebooks', nb_path)\n",
        "\n",
        "# package_path = '/content/drive/MyDrive/Colab_Notebooks/packages'\n",
        "# sys.path.insert(0,nb_path)\n",
        "# sys.path.insert(0,package_path)\n",
        "\n",
        "cur_path = os.path.join('/content/drive/MyDrive/Conf_Call/')\n",
        "print(os.getcwd())\n",
        "os.chdir(cur_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ArSJvtVIJF8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb2c543-a954-45c7-9f3c-1bcb0d551cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.8/dist-packages (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "import random, pickle\n",
        "import numpy as np\n",
        "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
        "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score, precision_recall_fscore_support\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "!pip install transformers\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "import copy\n",
        "from sklearn.utils import shuffle\n",
        "import glob\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0X48D4v42vH",
        "outputId": "adf542b6-c228-43e3-ec1c-26c6efb3b0aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 29 21:26:42 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    32W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEfSbAA4QHas",
        "outputId": "c497cda5-d6ad-40cd-cb80-d8df69e8d501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    device_name =\"/cpu:0\"\n",
        "    print('GPU device not found')\n",
        "    #raise SystemError('GPU device not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqG7FzRVFEIv"
      },
      "source": [
        "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYsV4H8fCpZ-",
        "outputId": "239710b4-1702-42a3-e36a-ce0a7b6119ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available(): \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6CJaQXuL0BD",
        "outputId": "1cca0938-19b2-4527-f8f6-1ca03e77b340"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fd841749530>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XygCjrnuL0Kn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rs3nVLsyRcJ"
      },
      "source": [
        "Download BioBERT Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JrUHXms16cn"
      },
      "source": [
        "# 2. Parse data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYWzeGSY2xh3"
      },
      "source": [
        "We'll use pandas to parse the \"in-domain\" training set and look at a few of its properties and data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "_UkeC7SG2krJ",
        "outputId": "d4423677-88ec-4962-b378-828e05fcc0a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences: 1,173\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               sentence  label\n",
              "506   And you are targeting, it sounds like multiple...      0\n",
              "220   Unusual to see CN change guidance this early i...      1\n",
              "1043  And then real briefly on TiO2 feedstock pricin...      0\n",
              "977   I think you mentioned hundreds of units from o...      0\n",
              "174   It??s amazing that there is no common period e...      1\n",
              "584   And finally, I got to ask Atish what??s the bi...      0\n",
              "786   If I was reading the document correctly, it lo...      1\n",
              "1139  So, it wouldn??t have been a material contribu...      0\n",
              "58    I'm wondering why you think we're not seeing i...      1\n",
              "240   And you referred that the last time you saw th...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d68a7a8e-fb7a-4af9-9fd3-ed23f1fcca93\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>506</th>\n",
              "      <td>And you are targeting, it sounds like multiple...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>Unusual to see CN change guidance this early i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1043</th>\n",
              "      <td>And then real briefly on TiO2 feedstock pricin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>977</th>\n",
              "      <td>I think you mentioned hundreds of units from o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>It??s amazing that there is no common period e...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>584</th>\n",
              "      <td>And finally, I got to ask Atish what??s the bi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>786</th>\n",
              "      <td>If I was reading the document correctly, it lo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1139</th>\n",
              "      <td>So, it wouldn??t have been a material contribu...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>I'm wondering why you think we're not seeing i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>And you referred that the last time you saw th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d68a7a8e-fb7a-4af9-9fd3-ed23f1fcca93')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d68a7a8e-fb7a-4af9-9fd3-ed23f1fcca93 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d68a7a8e-fb7a-4af9-9fd3-ed23f1fcca93');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/temp/merge_qa_label.csv\", encoding=\"ISO-8859-1\")\n",
        "#df = pd.read_csv(\"surprise_checking_internal_0905.csv\", encoding=\"ISO-8859-1\")\n",
        "#df = df[df.Negative==0]\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "df = df.drop(['Unnamed: 0','Unnamed: 2'], axis=1)\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)\n",
        "\n",
        "# df = pd.read_excel(\"/content/drive/MyDrive/temp/surprise_dt_test_v9_all_kiera.xlsx\")\n",
        "# #df = df[df.Negative==0]\n",
        "# # Report the number of sentences.\n",
        "# print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "# df = df.drop(['Unnamed: 0'], axis=1)\n",
        "# df = df.rename(columns={'merged':'label'})\n",
        "# # Display 10 random rows from the data.\n",
        "# df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(3)"
      ],
      "metadata": {
        "id": "nlJIz3HcCqQx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ecjszS0spgyL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OubtD2dURIfo",
        "outputId": "4121c70c-bae8-43f4-9a0f-c279676fc5d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1173"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    928\n",
              "1    245\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(df)\n",
        "#df[\"label\"] =df[\"label\"].fillna(0)\n",
        "df= df[~df['label'].isna()]\n",
        "\n",
        "df.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icpDUjP7Bu4f",
        "outputId": "5a9ba6fc-e556-4ab0-df05-07515144bf66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    280\n",
              "1    245\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "neg = 280\n",
        "import sklearn\n",
        "negs = sklearn.utils.shuffle(df[df.label==0].index.tolist())\n",
        "df = df[(df.label==1) | (df.index.isin(negs[0:neg]))]\n",
        "\n",
        "df.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.label==0].iloc[0].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfEM9QjFB3MM",
        "outputId": "5dc6e0d6-4a42-4127-c126-f354b152e164"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['But I??m just wondering how you actually manage that, when you??re looking at your underwriting teams and trying to manage their risks properly around a business that??s growing at such a high rate.',\n",
              "       0], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SMZ5T5Imhlx"
      },
      "source": [
        "\n",
        "\n",
        "Let's extract the sentences and labels of our training set as numpy ndarrays."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/temp/traingsample.csv')"
      ],
      "metadata": {
        "id": "bwLfYDEg6GbL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuE5BqICAne2",
        "outputId": "fa040530-01a0-4982-914c-14ce33b71801"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "245"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.sentence.values\n",
        "labels = [0,1]\n",
        "num_labels = len(labels)\n",
        "df.label.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cnyOs8zP8YO",
        "outputId": "20d7a5bb-4a5c-4fd0-f5df-d9c4e449882a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "525\n"
          ]
        }
      ],
      "source": [
        "labels[0:2]\n",
        "print(len(labels))\n",
        "print(len(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df.to_csv('/content/drive/MyDrive/temp/traingsample.csv',index=False)"
      ],
      "metadata": {
        "id": "8U6RHQC5Ro3V"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5O1eV-Pfct"
      },
      "source": [
        "# 3. Tokenization & Input Formatting\n",
        "\n",
        "In this section, we'll transform our dataset into the format that BERT can be trained on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWOPOyWghJp2"
      },
      "source": [
        "\n",
        "To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n",
        "\n",
        "The tokenization must be performed by the tokenizer included with BERT--the below cell will download this for us. We'll be using the \"uncased\" version here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z474sSC6oe7A",
        "outputId": "1e6c2c24-57c0-4c3c-9cb9-8f0c281a1223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ],
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained('yiyanghkust/finbert-pretrain', do_lower_case=True )\n",
        "# tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert', do_lower_case=True )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFzmtleW6KmJ"
      },
      "source": [
        "Let's apply the tokenizer to one sentence just to see the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KQO0EzmI-Ck",
        "outputId": "557e065c-b2fa-438a-a7df-03ff26e5be49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  But I??m just wondering how you actually manage that, when you??re looking at your underwriting teams and trying to manage their risks properly around a business that??s growing at such a high rate.\n",
            "Tokenized:  ['but', 'i', '?', '?', 'm', 'just', 'wondering', 'how', 'you', 'actually', 'manage', 'that', ',', 'when', 'you', '?', '?', 're', 'looking', 'at', 'your', 'under', '##writing', 'teams', 'and', 'trying', 'to', 'manage', 'their', 'risks', 'properly', 'around', 'a', 'business', 'that', '?', '?', 's', 'growing', 'at', 'such', 'a', 'high', 'rate', '.']\n",
            "Token IDs:  [2021, 1045, 1029, 1029, 1049, 2074, 6603, 2129, 2017, 2941, 6133, 2008, 1010, 2043, 2017, 1029, 1029, 2128, 2559, 2012, 2115, 2104, 18560, 2780, 1998, 2667, 2000, 6133, 2037, 10831, 7919, 2105, 1037, 2449, 2008, 1029, 1029, 1055, 3652, 2012, 2107, 1037, 2152, 3446, 1012]\n"
          ]
        }
      ],
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxP_VbyCsjXN",
        "outputId": "7e57d74a-7fca-49e7-8055-8140da56d0cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "\n",
        "bert_model = AutoModel.from_pretrained(\n",
        "    # 'ProsusAI/finbert',\n",
        "    'bert-base-uncased',\n",
        "    num_labels = 2, \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "    )\n",
        "bert_model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "I8WiOzGYTyEW"
      },
      "outputs": [],
      "source": [
        "# Put everything together as a function. This is for pretrained word vectors\n",
        "\n",
        "def get_pretrained_wordvector(sentences, tokenizer, bert_model):\n",
        "\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    max_len =100\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = max_len,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        #padding='max_length',\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "        # Add the encoded sentence to the list.    \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "        # And its attention mask (simply differentiates padding from non-padding).\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "\n",
        "    # Convert the lists into tensors.\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    bert_model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        outputs = bert_model(input_ids.to(device), attention_masks.to(device))   \n",
        "        hidden_states = outputs[2]\n",
        "\n",
        "    \n",
        "    # get the last four layers\n",
        "    token_embeddings = torch.stack(hidden_states[-4:], dim=0) \n",
        "    #print(token_embeddings.size())\n",
        "\n",
        "    # permute axis\n",
        "    token_embeddings = token_embeddings.permute(1,2,0,3)\n",
        "    #print(token_embeddings.size())\n",
        "\n",
        "    # take the mean of the last 4 layers\n",
        "    token_embeddings = token_embeddings.mean(axis=2)\n",
        "\n",
        "    #print(token_embeddings.size())\n",
        "\n",
        "    return token_embeddings, attention_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDpy4z_hVKG2",
        "outputId": "847f509f-7a24-46eb-f7ca-9edb031c7cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([525, 100])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings, masks = get_pretrained_wordvector(sentences, tokenizer, bert_model)\n",
        "print(masks.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EQvRzM10sped",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ad8503-f072-4701-dbc9-ed23cceeec4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([525, 100, 768])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embeddings.to(device) * masks.unsqueeze(-1).to(device)\n",
        "print(token_embeddings.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxJ1ZA4etjHG"
      },
      "source": [
        "# 4. Define model\n",
        "\n",
        "\n",
        "The model has two layers:\n",
        "BiLSTM\n",
        "CNN\n",
        "Dense Layer\n",
        "\n",
        "Depending on loss function used, this model can be single-label or multi-label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "exhSQc_7tnqz"
      },
      "outputs": [],
      "source": [
        "class cnn(nn.Module):\n",
        "\n",
        "    # define all the layers used in model\n",
        "    def __init__(self, emb_dim, seq_len, num_filters, kernel_sizes, num_classes, dropout_rate = 0.5):\n",
        "      \n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.seq_len = seq_len\n",
        "        \n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        #self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_index)\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1,self.num_filters, (f, self.emb_dim)) for f in self.kernel_sizes])\n",
        "        self.fc = nn.Linear(len(kernel_sizes)*self.num_filters, self.num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x, _ = self.lstm(x)  # (N, seq_len, 2*lstm_units)\n",
        "\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        #print(x.size())\n",
        "\n",
        "        x = [F.relu(conv(x).squeeze(-1)) for conv in self.convs]  # output of three conv\n",
        "\n",
        "        #print(x[0].size())\n",
        "\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] # continue with 3 maxpooling\n",
        "\n",
        "        x = torch.cat(x, 1)  # N, len(filter_sizes)* num_filters\n",
        "        #print(x.size())\n",
        "\n",
        "        x = self.dropout(x)  # N, len(filter_sizes)* num_filters\n",
        "\n",
        "        logit = self.fc(x)  # (N, num_classes)\n",
        "\n",
        "        return logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kzlP7U3xU65k"
      },
      "outputs": [],
      "source": [
        "class lstm_cnn(nn.Module):\n",
        "\n",
        "    # define all the layers used in model\n",
        "    def __init__(self, emb_dim, seq_len, lstm_units, num_filters, kernel_sizes, num_classes, dropout_rate = 0.5):\n",
        "      \n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.lstm_units = lstm_units\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "\n",
        "        #self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_index)\n",
        "        # input: [1173, 100, 768]\n",
        "        # self.norm1 = torch.nn.LayerNorm([self.seq_len , self.emb_dim])\n",
        "        # self.norm1 = nn.BatchNorm1d(seq_len)\n",
        "        self.lstm = nn.LSTM(emb_dim,\n",
        "                            lstm_units,\n",
        "                            num_layers=1,\n",
        "                            bidirectional=True,\n",
        "                            batch_first=True)  # \n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1,self.num_filters, (f, 2*self.lstm_units)) for f in self.kernel_sizes])\n",
        "        # self.convs = nn.ModuleList([nn.Conv2d(1,self.num_filters, (f, self.lstm_units)) for f in self.kernel_sizes])\n",
        "        #self.norm2 = nn.BatchNorm1d(len(kernel_sizes)*self.num_filters)\n",
        "        self.fc = nn.Linear(len(kernel_sizes)*self.num_filters, self.num_classes)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "       # x = self.norm1(x)\n",
        "\n",
        "        x, _ = self.lstm(x)  # (N, seq_len, 2*lstm_units)\n",
        "\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        #print(x.size())\n",
        "\n",
        "        x = [F.relu(conv(x).squeeze(-1)) for conv in self.convs]  # output of three conv\n",
        "\n",
        "        #print(x[0].size())\n",
        "\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] # continue with 3 maxpooling\n",
        "\n",
        "        x = torch.cat(x, 1)  # N, len(filter_sizes)* num_filters\n",
        "        #print(x.size())\n",
        "\n",
        "        #x = self.norm2(x)\n",
        "\n",
        "        x = self.dropout(x)  # N, len(filter_sizes)* num_filters\n",
        "\n",
        "        logit = self.fc(x)  # (N, num_classes)\n",
        "\n",
        "        return logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNklenpst7LL",
        "outputId": "fdbe9bbc-7568-4cd5-ba83-f5c3a54c74b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "cnn                                      [32, 6]                   --\n",
              "ModuleList: 1-1                        --                        --\n",
              "    Conv2d: 2-1                       [32, 32, 100, 1]          3,232\n",
              "    Conv2d: 2-2                       [32, 32, 99, 1]           6,432\n",
              "    Conv2d: 2-3                       [32, 32, 98, 1]           9,632\n",
              "Dropout: 1-2                           [32, 96]                  --\n",
              "Linear: 1-3                            [32, 6]                   582\n",
              "==========================================================================================\n",
              "Total params: 19,878\n",
              "Trainable params: 19,878\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 60.94\n",
              "==========================================================================================\n",
              "Input size (MB): 1.28\n",
              "Forward/backward pass size (MB): 2.43\n",
              "Params size (MB): 0.08\n",
              "Estimated Total Size (MB): 3.79\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "model = cnn(100, 100, 32, [1,2,3], 6)\n",
        "#summary(model.to(device),(32, 100, 100))\n",
        "summary(model,(32, 100, 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M9jwsr4qorx"
      },
      "source": [
        "#6. **Define a function to train single-label classifier**\n",
        "\n",
        "The loss function is different from multi-label classifer\n",
        "\n",
        "Parameters:\n",
        "\n",
        "* model: model defined\n",
        "*   num_labels: number of labels\n",
        "*   label_cols: label names\n",
        "*   train_dataloader: train data loader\n",
        "*   validation_dataloader: validation data loader\n",
        "*   optimizer: optimizer. default is Adam\n",
        "*   scheduler: adjust learning rate dynamically; default is None.\n",
        "*   epochs: number of epochs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paxPyzYb7Emm"
      },
      "source": [
        "### 6.1. Evalution Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9ELmiqyd35w8"
      },
      "outputs": [],
      "source": [
        "def model_eval(model, dataloader, class_weight = None):\n",
        "  tokenized_texts = []\n",
        "  true_labels = []\n",
        "  pred_labels = []\n",
        "\n",
        "  threshold = 0.5\n",
        "\n",
        "  total_eval_accuracy = 0\n",
        "  total_eval_loss = 0\n",
        "\n",
        "  for batch in validation_dataloader:\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_labels = batch[1].to(device)\n",
        "\n",
        "    with torch.no_grad():        \n",
        "\n",
        "      logits = model(b_input_ids)\n",
        "      #loss_func = BCELoss()\n",
        "      #val_loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
        "\n",
        "      if class_weight != None:\n",
        "          pos_weight=torch.tensor(class_weight).to(device)\n",
        "          loss_func = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "      else:\n",
        "          loss_func = BCEWithLogitsLoss()\n",
        "\n",
        "      val_loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation          \n",
        "            \n",
        "      total_eval_loss += val_loss.item()\n",
        "    \n",
        "      pred_label = torch.sigmoid(logits)   \n",
        "      b_labels = b_labels.to('cpu').numpy()\n",
        "      pred_label = pred_label.to('cpu').numpy()\n",
        "      \n",
        "      tokenized_texts.append(b_input_ids)\n",
        "      true_labels.append(b_labels)\n",
        "      pred_labels.append(pred_label)\n",
        "\n",
        "    \n",
        "  # Flatten outputs\n",
        "  pred_labels = np.vstack(pred_labels)\n",
        "  true_labels = np.vstack(true_labels)\n",
        "\n",
        "  avg_val_loss = total_eval_loss / len(dataloader)    \n",
        "\n",
        "  return tokenized_texts, pred_labels, true_labels,avg_val_loss\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZAROYal7AfW"
      },
      "source": [
        "##6.2. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "dwcDj_XyqnIb"
      },
      "outputs": [],
      "source": [
        "def train_single_label_model(model, num_labels, label_cols, train_dataloader, validation_dataloader, model_path,\\\n",
        "                             optimizer=None, scheduler=None, epochs = 10, \\\n",
        "                             class_weight = None, patience = 5):\n",
        "\n",
        "    seed_val = 42\n",
        "\n",
        "    threshold = 0.5\n",
        "    #model_path = 'best_model.model'  # save the best model\n",
        "\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "    training_stats = []\n",
        "    \n",
        "    best_score = -0.5\n",
        "    best_epoch = 0\n",
        "    cnt = 0\n",
        "\n",
        "    total_t0 = time.time()\n",
        "\n",
        "    if optimizer==None:\n",
        "        optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "    # For each epoch...\n",
        "    for epoch_i in range(0, epochs):\n",
        "        \n",
        "        # ========================================\n",
        "        #               Training\n",
        "        # ========================================\n",
        "        \n",
        "        # Perform one full pass over the training set.\n",
        "\n",
        "        #print(\"\")\n",
        "        #print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "        #print('Training...')\n",
        "\n",
        "        # Measure how long the training epoch takes.\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Reset the total loss for this epoch.\n",
        "        total_train_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            # Progress update every 40 batches.\n",
        "            #if step % 40 == 0 and not step == 0:\n",
        "                # Calculate elapsed time in minutes.\n",
        "            #    elapsed = format_time(time.time() - t0)\n",
        "                \n",
        "                # Report progress.\n",
        "                #print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "            # Unpack this training batch from our dataloader. \n",
        "            #\n",
        "            # `batch` contains three pytorch tensors:\n",
        "            #   [0]: input ids \n",
        "            #   [2]: labels \n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_labels = batch[1].to(device)\n",
        "            \n",
        "            model.zero_grad()        \n",
        "\n",
        "            logits = model(b_input_ids)\n",
        "            #print(\"logits shape: \", b_input_ids.size(), b_labels.size(), logits.shape())\n",
        "            #loss_func = BCELoss()\n",
        "            #loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
        "\n",
        "            # add class weight\n",
        "            if class_weight != None:\n",
        "              pos_weight=torch.tensor(class_weight).to(device)\n",
        "              loss_func = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "            else:\n",
        "              loss_func = BCEWithLogitsLoss()\n",
        "\n",
        "            loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
        "            \n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the learning rate.\n",
        "            if scheduler!=None:\n",
        "                scheduler.step()\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "        \n",
        "        # Measure how long this epoch took.\n",
        "        #training_time = format_time(time.time() - t0)\n",
        "\n",
        "        #print(\"\")\n",
        "        #print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        #print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "            \n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "        # After the completion of each training epoch, measure our performance on\n",
        "        # our validation set.\n",
        "\n",
        "        #print(\"\")\n",
        "        #print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Put the model in evaluation mode--the dropout layers behave differently\n",
        "        # during evaluation.\n",
        "        model.eval()\n",
        "\n",
        "        tokenized_texts, pred_labels, true_labels,avg_val_loss = model_eval(model, validation_dataloader, class_weight = class_weight)\n",
        "\n",
        "        pred_bools = np.argmax(pred_labels, axis = 1)\n",
        "        true_bools = np.argmax(true_labels, axis = 1)\n",
        " \n",
        "        val_f1 = f1_score(true_bools,pred_bools, average = None)*100 \n",
        "        val_f1 = val_f1[1] # return f1 for  class 1\n",
        "        val_acc = (pred_bools == true_bools).astype(int).sum()/len(pred_bools)\n",
        "\n",
        "        #print('Validation Accuracy: {0:.4f}, F1: {1:.4f}, Loss: {2:.4f}'.format(val_f1, val_acc, avg_val_loss))\n",
        "        #print(classification_report(np.array(true_labels), pred_bools, target_names=label_cols) )\n",
        "        print(\"Epoch {0}\\t Train Loss: {1:.4f}\\t Val Loss {2:.4f}\\t Val Acc: {3:.4f}\\t Val F1: {4:.4f}\".\\\n",
        "          format(epoch_i +1, avg_train_loss, avg_val_loss, val_acc, val_f1))\n",
        "\n",
        "        # Measure how long the validation run took.\n",
        "        #validation_time = format_time(time.time() - t0)\n",
        "        \n",
        "        #print(\"  Validation Loss: {0:.2f}\".format(val_f1_accuracy))\n",
        "        #print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "        # Record all statistics from this epoch.\n",
        "        training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Training Loss': avg_train_loss,\n",
        "                'Valid. Loss': avg_val_loss,\n",
        "                'Valid. Accur.': val_f1,\n",
        "                'Best F1': best_score,\n",
        "                'Best epoch': best_epoch\n",
        "                #'Training Time': training_time,\n",
        "                #'Validation Time': validation_time\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # early stopping\n",
        "        if val_f1 > best_score:\n",
        "            best_score = val_f1\n",
        "            best_epoch = epoch_i + 1\n",
        "            torch.save(copy.deepcopy(model.state_dict()), model_path)\n",
        "            print(\"model saved\")\n",
        "            cnt = 0\n",
        "        else:\n",
        "            cnt += 1\n",
        "            if cnt == patience:\n",
        "                print(\"\\n\")\n",
        "                print(\"early stopping at epoch {0}\".format(epoch_i+1))\n",
        "\n",
        "                break\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    print(\"\")\n",
        "    #print(\"Training complete!\")\n",
        "\n",
        "    #print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "    return model, training_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEeNyw0Keo4y"
      },
      "source": [
        "## 6.3. 4-fold cross validation; one-vs-the-rest\n",
        "Train single label classifier using one vs. the rest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcExaiA9WeWu",
        "outputId": "4a3e3516-52a4-4b6d-81cf-90738cfd4a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "525\n"
          ]
        }
      ],
      "source": [
        "sentences = df.sentence.values\n",
        "print(len(sentences))\n",
        "#labels = list(df1.one_hot_labels.values)\n",
        "#num_labels = len(label_cols)\n",
        "\n",
        "vectors, masks = get_pretrained_wordvector(sentences, tokenizer, bert_model) \n",
        "vectors = vectors.to(device) * masks.unsqueeze(-1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XCICxXmenoB",
        "outputId": "98c2b5ab-b4f1-47dd-f67e-423f82edbad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------\n",
            "label\n",
            "------------\n",
            "\n",
            "fold 0 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lstm_cnn(\n",
              "  (lstm): LSTM(768, 100, batch_first=True, bidirectional=True)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 64, kernel_size=(1, 200), stride=(1, 1))\n",
              "    (1): Conv2d(1, 64, kernel_size=(3, 200), stride=(1, 1))\n",
              "    (2): Conv2d(1, 64, kernel_size=(5, 200), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=192, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\t Train Loss: 0.8359\t Val Loss 0.7895\t Val Acc: 0.7029\t Val F1: 71.7391\n",
            "model saved\n",
            "Epoch 2\t Train Loss: 0.7267\t Val Loss 0.6823\t Val Acc: 0.7143\t Val F1: 73.1183\n",
            "model saved\n",
            "Epoch 3\t Train Loss: 0.5590\t Val Loss 0.6534\t Val Acc: 0.7429\t Val F1: 71.3376\n",
            "Epoch 4\t Train Loss: 0.4035\t Val Loss 0.7396\t Val Acc: 0.7657\t Val F1: 73.5484\n",
            "model saved\n",
            "Epoch 5\t Train Loss: 0.2382\t Val Loss 0.8303\t Val Acc: 0.7771\t Val F1: 75.4717\n",
            "model saved\n",
            "Epoch 6\t Train Loss: 0.1317\t Val Loss 1.2708\t Val Acc: 0.6914\t Val F1: 69.3182\n",
            "Epoch 7\t Train Loss: 0.1172\t Val Loss 1.3336\t Val Acc: 0.7657\t Val F1: 72.8477\n",
            "Epoch 8\t Train Loss: 0.0516\t Val Loss 1.2980\t Val Acc: 0.7657\t Val F1: 73.5484\n",
            "Epoch 9\t Train Loss: 0.0164\t Val Loss 1.5635\t Val Acc: 0.7543\t Val F1: 70.7483\n",
            "Epoch 10\t Train Loss: 0.0223\t Val Loss 1.9626\t Val Acc: 0.7029\t Val F1: 70.4545\n",
            "\n",
            "\n",
            "early stopping at epoch 10\n",
            "\n",
            "load the best model ... \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lstm_cnn(\n",
              "  (lstm): LSTM(768, 100, batch_first=True, bidirectional=True)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 64, kernel_size=(1, 200), stride=(1, 1))\n",
              "    (1): Conv2d(1, 64, kernel_size=(3, 200), stride=(1, 1))\n",
              "    (2): Conv2d(1, 64, kernel_size=(5, 200), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=192, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.7692, Recall: 0.7407, F1: 0.7547, Loss: 0.8303\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.81      0.80        94\n",
            "           1       0.77      0.74      0.75        81\n",
            "\n",
            "    accuracy                           0.78       175\n",
            "   macro avg       0.78      0.77      0.78       175\n",
            "weighted avg       0.78      0.78      0.78       175\n",
            "\n",
            "\n",
            "fold 1 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lstm_cnn(\n",
              "  (lstm): LSTM(768, 100, batch_first=True, bidirectional=True)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 64, kernel_size=(1, 200), stride=(1, 1))\n",
              "    (1): Conv2d(1, 64, kernel_size=(3, 200), stride=(1, 1))\n",
              "    (2): Conv2d(1, 64, kernel_size=(5, 200), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=192, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\t Train Loss: 0.8161\t Val Loss 0.7753\t Val Acc: 0.6286\t Val F1: 70.0461\n",
            "model saved\n",
            "Epoch 2\t Train Loss: 0.6828\t Val Loss 0.7033\t Val Acc: 0.6514\t Val F1: 71.3615\n",
            "model saved\n",
            "Epoch 3\t Train Loss: 0.5690\t Val Loss 0.6411\t Val Acc: 0.7314\t Val F1: 75.3927\n",
            "model saved\n",
            "Epoch 4\t Train Loss: 0.3852\t Val Loss 0.6921\t Val Acc: 0.7429\t Val F1: 73.3728\n",
            "Epoch 5\t Train Loss: 0.2219\t Val Loss 1.0842\t Val Acc: 0.6800\t Val F1: 62.1622\n",
            "Epoch 6\t Train Loss: 0.1103\t Val Loss 1.3720\t Val Acc: 0.7257\t Val F1: 68.0000\n",
            "Epoch 7\t Train Loss: 0.0297\t Val Loss 1.7587\t Val Acc: 0.7086\t Val F1: 67.0968\n",
            "Epoch 8\t Train Loss: 0.0179\t Val Loss 1.8494\t Val Acc: 0.7257\t Val F1: 72.0930\n",
            "\n",
            "\n",
            "early stopping at epoch 8\n",
            "\n",
            "load the best model ... \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lstm_cnn(\n",
              "  (lstm): LSTM(768, 100, batch_first=True, bidirectional=True)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 64, kernel_size=(1, 200), stride=(1, 1))\n",
              "    (1): Conv2d(1, 64, kernel_size=(3, 200), stride=(1, 1))\n",
              "    (2): Conv2d(1, 64, kernel_size=(5, 200), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=192, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.6606, Recall: 0.8780, F1: 0.7539, Loss: 0.6411\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.60      0.70        93\n",
            "           1       0.66      0.88      0.75        82\n",
            "\n",
            "    accuracy                           0.73       175\n",
            "   macro avg       0.75      0.74      0.73       175\n",
            "weighted avg       0.76      0.73      0.73       175\n",
            "\n",
            "\n",
            "fold 2 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lstm_cnn(\n",
              "  (lstm): LSTM(768, 100, batch_first=True, bidirectional=True)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 64, kernel_size=(1, 200), stride=(1, 1))\n",
              "    (1): Conv2d(1, 64, kernel_size=(3, 200), stride=(1, 1))\n",
              "    (2): Conv2d(1, 64, kernel_size=(5, 200), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=192, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\t Train Loss: 0.8144\t Val Loss 0.7718\t Val Acc: 0.5029\t Val F1: 64.4898\n",
            "model saved\n",
            "Epoch 2\t Train Loss: 0.6610\t Val Loss 0.6321\t Val Acc: 0.7543\t Val F1: 70.3448\n",
            "model saved\n",
            "Epoch 3\t Train Loss: 0.4731\t Val Loss 0.5990\t Val Acc: 0.7257\t Val F1: 72.0930\n",
            "model saved\n",
            "Epoch 4\t Train Loss: 0.2964\t Val Loss 0.6862\t Val Acc: 0.7371\t Val F1: 70.8861\n",
            "Epoch 5\t Train Loss: 0.1494\t Val Loss 0.9759\t Val Acc: 0.6914\t Val F1: 63.0137\n",
            "Epoch 6\t Train Loss: 0.1119\t Val Loss 1.1470\t Val Acc: 0.7143\t Val F1: 62.6866\n",
            "Epoch 7\t Train Loss: 0.0640\t Val Loss 1.2790\t Val Acc: 0.7143\t Val F1: 65.2778\n",
            "Epoch 8\t Train Loss: 0.0110\t Val Loss 1.4754\t Val Acc: 0.6971\t Val F1: 61.8705\n",
            "\n",
            "\n",
            "early stopping at epoch 8\n",
            "\n",
            "load the best model ... \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lstm_cnn(\n",
              "  (lstm): LSTM(768, 100, batch_first=True, bidirectional=True)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 64, kernel_size=(1, 200), stride=(1, 1))\n",
              "    (1): Conv2d(1, 64, kernel_size=(3, 200), stride=(1, 1))\n",
              "    (2): Conv2d(1, 64, kernel_size=(5, 200), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=192, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.6889, Recall: 0.7561, F1: 0.7209, Loss: 0.5990\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.70      0.73        93\n",
            "           1       0.69      0.76      0.72        82\n",
            "\n",
            "    accuracy                           0.73       175\n",
            "   macro avg       0.73      0.73      0.73       175\n",
            "weighted avg       0.73      0.73      0.73       175\n",
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "emb_dim = vectors.size(-1)\n",
        "seq_len = vectors.size(1)\n",
        "num_filters = 64\n",
        "kernel_sizes = [1, 3, 5]\n",
        "num_labels = 2\n",
        "labels = ['0','1']\n",
        "class_weight = [1.0,2.0]\n",
        "\n",
        "result = []\n",
        "label_cols = ['label']\n",
        "\n",
        "for col in label_cols:\n",
        "    print(\"\\n------------\") \n",
        "    print(col)\n",
        "    print(\"------------\")\n",
        "    \n",
        "    y = df[col].astype(int).values\n",
        "\n",
        "    fold = 0\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n",
        "    \n",
        "    for train_index, test_index in skf.split(vectors, y): \n",
        "\n",
        "        print(\"\\nfold {} \\n\".format(fold))\n",
        "\n",
        "        fold += 1\n",
        "        X_train, X_test = vectors[train_index], vectors[test_index]\n",
        "        Y_train, Y_test = y[train_index], y[test_index]\n",
        "\n",
        "        Y_train = pd.get_dummies(Y_train).values\n",
        "        Y_train = torch.tensor(Y_train)\n",
        "\n",
        "        Y_test = pd.get_dummies(Y_test).values\n",
        "        Y_test = torch.tensor(Y_test)\n",
        "\n",
        "        train_dataset = TensorDataset(X_train, Y_train)\n",
        "        val_dataset = TensorDataset(X_test, Y_test)\n",
        "\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "        validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "        #weight = 10\n",
        "        #train_sample_weight = np.array([weight if i ==1 else 1 for i in Y_train])\n",
        "        #test_sample_weight = np.array([weight if i ==1 else 1 for i in Y_test])\n",
        "\n",
        "        model_name =  \"/content/drive/MyDrive/temp/bert_model/model_\" + str(fold)\n",
        "        #model = cnn(emb_dim, seq_len, num_filters, kernel_sizes, num_labels)\n",
        "        model = lstm_cnn(emb_dim, seq_len, 100, \\\n",
        "                         num_filters, kernel_sizes, num_labels)\n",
        "        model.to(device)\n",
        "\n",
        "\n",
        "        model, training_stats = train_single_label_model(model, num_labels, labels, train_dataloader, validation_dataloader, \\\n",
        "                                                         model_path = model_name, class_weight = class_weight,\n",
        "                                                        optimizer=None, scheduler=None, epochs = 20)\n",
        "        \n",
        "        print(\"load the best model ... \")\n",
        "\n",
        "        model.load_state_dict(torch.load(model_name))\n",
        "\n",
        "        # show performance of best model\n",
        "        model.eval()\n",
        "        tokenized_texts, pred_labels, true_labels,avg_val_loss = model_eval(model, validation_dataloader, class_weight = class_weight)\n",
        "\n",
        "        pred_bools = np.argmax(pred_labels, axis = 1)\n",
        "        true_bools = np.argmax(true_labels, axis = 1)\n",
        "\n",
        "        p, r, f, _ = precision_recall_fscore_support(true_bools,pred_bools, pos_label = 1)\n",
        "        #val_f1 = f1_score(true_bools,pred_bools, average = None)*100 \n",
        "        #val_f1 = val_f1[1] # return f1 for  class 1\n",
        "        val_acc = (pred_bools == true_bools).astype(int).sum()/len(pred_bools)\n",
        "   \n",
        "    \n",
        "        print('Precision: {0:.4f}, Recall: {1:.4f}, F1: {2:.4f}, Loss: {3:.4f}'.format(p[1], r[1], f[1], avg_val_loss))\n",
        "        print(classification_report(true_bools, pred_bools) )\n",
        "\n",
        "        \n",
        "    \n",
        "        #p, r, f = train_model(model, X_train, Y_train, train_sample_weight,\\\n",
        "        #                   X_test, Y_test, test_sample_weight, \\\n",
        "        #                   'baseline_models/lstm_cnn/'+col)\n",
        "\n",
        "        result.append([col, fold, p[1], r[1], f[1], training_stats[-1][\"Best epoch\"]])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCo2gdbFOuIm",
        "outputId": "ca464914-274e-4ad3-bb37-73ab77bff288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "   precision    recall        f1  epoch\n",
            "0   0.769231  0.740741  0.754717      5\n",
            "1   0.660550  0.878049  0.753927      3\n",
            "2   0.688889  0.756098  0.720930      3\n",
            " \n",
            "       precision    recall        f1     epoch\n",
            "label                                         \n",
            "label   0.706223  0.791629  0.743191  3.666667\n"
          ]
        }
      ],
      "source": [
        "result_df = pd.DataFrame(result, columns =[\"label\",\"fold\",\"precision\",\"recall\",\"f1\",\"epoch\"])\n",
        "\n",
        "for col in label_cols:\n",
        "    print(col)\n",
        "    print(result_df[result_df.label == col][[\"precision\",\"recall\",\"f1\",\"epoch\"]])\n",
        "    print(\" \")\n",
        "print(result_df[[\"label\",\"precision\",\"recall\",\"f1\",\"epoch\"]].groupby(\"label\").mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "h4n2alcNZ_hW",
        "outputId": "69e44c51-5605-4727-f65d-c68308498c75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd77e650b50>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "loss = [[i, item['Training Loss'], item['Valid. Loss']] for i, item in enumerate(training_stats)]\n",
        "acc = [[item[\"Best epoch\"], 'Valid. Accur.'] for item in training_stats]\n",
        "\n",
        "pd.DataFrame(loss, columns=[\"epoch\", \"train_loss\",\"val_loss\"]).set_index(\"epoch\").plot(kind=\"line\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7e7UoiE1gjku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac251be-0406-4118-b8ac-f6f6cee5c77c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "525\n"
          ]
        }
      ],
      "source": [
        "sentences = df.sentence.values\n",
        "print(len(sentences))\n",
        "#labels = list(df1.one_hot_labels.values)\n",
        "#num_labels = len(label_cols)\n",
        "\n",
        "vectors, masks = get_pretrained_wordvector(sentences, tokenizer, bert_model) \n",
        "vectors =  vectors.to(device) * masks.unsqueeze(-1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "utiyYW9ud0ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d284e8-d5b3-4eb5-b602-84721fd96d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------\n",
            "label\n",
            "------------\n",
            "\n",
            "fold 0 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lstm_cnn(\n",
              "  (lstm): LSTM(768, 100, batch_first=True, bidirectional=True)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 64, kernel_size=(1, 200), stride=(1, 1))\n",
              "    (1): Conv2d(1, 64, kernel_size=(3, 200), stride=(1, 1))\n",
              "    (2): Conv2d(1, 64, kernel_size=(5, 200), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=192, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\t Train Loss: 0.6835\t Val Loss 0.6512\t Val Acc: 0.6457\t Val F1: 62.6506\n",
            "model saved\n",
            "Epoch 2\t Train Loss: 0.5491\t Val Loss 0.5558\t Val Acc: 0.7371\t Val F1: 65.1515\n",
            "model saved\n",
            "Epoch 3\t Train Loss: 0.4311\t Val Loss 0.6159\t Val Acc: 0.7143\t Val F1: 63.7681\n",
            "Epoch 4\t Train Loss: 0.2829\t Val Loss 0.6839\t Val Acc: 0.7086\t Val F1: 66.6667\n",
            "model saved\n",
            "Epoch 5\t Train Loss: 0.1481\t Val Loss 0.8445\t Val Acc: 0.7200\t Val F1: 65.7343\n",
            "Epoch 6\t Train Loss: 0.0728\t Val Loss 1.0153\t Val Acc: 0.7143\t Val F1: 65.7534\n",
            "Epoch 7\t Train Loss: 0.0167\t Val Loss 1.2825\t Val Acc: 0.6743\t Val F1: 65.8683\n",
            "Epoch 8\t Train Loss: 0.0049\t Val Loss 1.6906\t Val Acc: 0.6514\t Val F1: 66.2983\n",
            "Epoch 9\t Train Loss: 0.0027\t Val Loss 1.7501\t Val Acc: 0.6743\t Val F1: 66.2722\n",
            "\n",
            "\n",
            "early stopping at epoch 9\n",
            "\n",
            "load the best model ... \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lstm_cnn(\n",
              "  (lstm): LSTM(768, 100, batch_first=True, bidirectional=True)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 64, kernel_size=(1, 200), stride=(1, 1))\n",
              "    (1): Conv2d(1, 64, kernel_size=(3, 200), stride=(1, 1))\n",
              "    (2): Conv2d(1, 64, kernel_size=(5, 200), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=192, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.7083, Recall: 0.6296, F1: 0.6667, Loss: 0.6839\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.78      0.74        94\n",
            "           1       0.71      0.63      0.67        81\n",
            "\n",
            "    accuracy                           0.71       175\n",
            "   macro avg       0.71      0.70      0.70       175\n",
            "weighted avg       0.71      0.71      0.71       175\n",
            "\n",
            "\n",
            "fold 1 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lstm_cnn(\n",
              "  (lstm): LSTM(768, 100, batch_first=True, bidirectional=True)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 64, kernel_size=(1, 200), stride=(1, 1))\n",
              "    (1): Conv2d(1, 64, kernel_size=(3, 200), stride=(1, 1))\n",
              "    (2): Conv2d(1, 64, kernel_size=(5, 200), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=192, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\t Train Loss: 0.6813\t Val Loss 0.6509\t Val Acc: 0.5657\t Val F1: 13.6364\n",
            "model saved\n",
            "Epoch 2\t Train Loss: 0.5912\t Val Loss 0.5304\t Val Acc: 0.7429\t Val F1: 73.3728\n",
            "model saved\n",
            "Epoch 3\t Train Loss: 0.4587\t Val Loss 0.5750\t Val Acc: 0.6857\t Val F1: 72.0812\n",
            "Epoch 4\t Train Loss: 0.3774\t Val Loss 0.5548\t Val Acc: 0.7257\t Val F1: 63.0769\n",
            "Epoch 5\t Train Loss: 0.2552\t Val Loss 0.5951\t Val Acc: 0.7314\t Val F1: 65.6934\n",
            "Epoch 6\t Train Loss: 0.1532\t Val Loss 0.7003\t Val Acc: 0.7200\t Val F1: 63.1579\n",
            "Epoch 7\t Train Loss: 0.0578\t Val Loss 0.8908\t Val Acc: 0.7314\t Val F1: 68.4564\n",
            "\n",
            "\n",
            "early stopping at epoch 7\n",
            "\n",
            "load the best model ... \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lstm_cnn(\n",
              "  (lstm): LSTM(768, 100, batch_first=True, bidirectional=True)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 64, kernel_size=(1, 200), stride=(1, 1))\n",
              "    (1): Conv2d(1, 64, kernel_size=(3, 200), stride=(1, 1))\n",
              "    (2): Conv2d(1, 64, kernel_size=(5, 200), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=192, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.7126, Recall: 0.7561, F1: 0.7337, Loss: 0.5304\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.73      0.75        93\n",
            "           1       0.71      0.76      0.73        82\n",
            "\n",
            "    accuracy                           0.74       175\n",
            "   macro avg       0.74      0.74      0.74       175\n",
            "weighted avg       0.74      0.74      0.74       175\n",
            "\n",
            "\n",
            "fold 2 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lstm_cnn(\n",
              "  (lstm): LSTM(768, 100, batch_first=True, bidirectional=True)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 64, kernel_size=(1, 200), stride=(1, 1))\n",
              "    (1): Conv2d(1, 64, kernel_size=(3, 200), stride=(1, 1))\n",
              "    (2): Conv2d(1, 64, kernel_size=(5, 200), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=192, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\t Train Loss: 0.6930\t Val Loss 0.6567\t Val Acc: 0.7200\t Val F1: 67.5497\n",
            "model saved\n",
            "Epoch 2\t Train Loss: 0.6090\t Val Loss 0.5507\t Val Acc: 0.7371\t Val F1: 65.1515\n",
            "Epoch 3\t Train Loss: 0.4823\t Val Loss 0.5151\t Val Acc: 0.7714\t Val F1: 72.6027\n",
            "model saved\n",
            "Epoch 4\t Train Loss: 0.3825\t Val Loss 0.5498\t Val Acc: 0.7257\t Val F1: 64.1791\n",
            "Epoch 5\t Train Loss: 0.2775\t Val Loss 0.5958\t Val Acc: 0.7714\t Val F1: 75.3086\n",
            "model saved\n",
            "Epoch 6\t Train Loss: 0.1716\t Val Loss 0.6730\t Val Acc: 0.7714\t Val F1: 75.9036\n",
            "model saved\n",
            "Epoch 7\t Train Loss: 0.0663\t Val Loss 0.9112\t Val Acc: 0.7543\t Val F1: 75.1445\n",
            "Epoch 8\t Train Loss: 0.0152\t Val Loss 1.0643\t Val Acc: 0.7886\t Val F1: 78.1065\n",
            "model saved\n",
            "Epoch 9\t Train Loss: 0.0049\t Val Loss 1.3976\t Val Acc: 0.7543\t Val F1: 75.4286\n",
            "Epoch 10\t Train Loss: 0.0031\t Val Loss 1.3853\t Val Acc: 0.7714\t Val F1: 75.6098\n",
            "Epoch 11\t Train Loss: 0.0013\t Val Loss 1.4547\t Val Acc: 0.7771\t Val F1: 76.6467\n",
            "Epoch 12\t Train Loss: 0.0061\t Val Loss 1.5373\t Val Acc: 0.7543\t Val F1: 70.3448\n",
            "Epoch 13\t Train Loss: 0.0390\t Val Loss 1.3724\t Val Acc: 0.7600\t Val F1: 73.0769\n",
            "\n",
            "\n",
            "early stopping at epoch 13\n",
            "\n",
            "load the best model ... \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lstm_cnn(\n",
              "  (lstm): LSTM(768, 100, batch_first=True, bidirectional=True)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 64, kernel_size=(1, 200), stride=(1, 1))\n",
              "    (1): Conv2d(1, 64, kernel_size=(3, 200), stride=(1, 1))\n",
              "    (2): Conv2d(1, 64, kernel_size=(5, 200), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=192, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.7586, Recall: 0.8049, F1: 0.7811, Loss: 1.0643\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.77      0.80        93\n",
            "           1       0.76      0.80      0.78        82\n",
            "\n",
            "    accuracy                           0.79       175\n",
            "   macro avg       0.79      0.79      0.79       175\n",
            "weighted avg       0.79      0.79      0.79       175\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# use our labeled data\n",
        "\n",
        "batch_size = 32\n",
        "emb_dim = vectors.size(-1)\n",
        "seq_len = vectors.size(1)\n",
        "num_filters = 64\n",
        "kernel_sizes = [1, 3, 5]\n",
        "num_labels = 2\n",
        "labels = ['0','1']\n",
        "class_weight = [1.0,1.0]\n",
        "\n",
        "result = []\n",
        "label_cols = ['label']\n",
        "\n",
        "for col in label_cols:\n",
        "    print(\"\\n------------\") \n",
        "    print(col)\n",
        "    print(\"------------\")\n",
        "    \n",
        "    y = df[col].astype(int).values\n",
        "\n",
        "    fold = 0\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=3, random_state=0, shuffle=True)\n",
        "    \n",
        "    for train_index, test_index in skf.split(vectors, y): \n",
        "\n",
        "        print(\"\\nfold {} \\n\".format(fold))\n",
        "\n",
        "        fold += 1\n",
        "        X_train, X_test = vectors[train_index], vectors[test_index]\n",
        "        Y_train, Y_test = y[train_index], y[test_index]\n",
        "\n",
        "        Y_train = pd.get_dummies(Y_train).values\n",
        "        Y_train = torch.tensor(Y_train)\n",
        "\n",
        "        Y_test = pd.get_dummies(Y_test).values\n",
        "        Y_test = torch.tensor(Y_test)\n",
        "\n",
        "        train_dataset = TensorDataset(X_train, Y_train)\n",
        "        val_dataset = TensorDataset(X_test, Y_test)\n",
        "\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "        validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "        #weight = 10\n",
        "        #train_sample_weight = np.array([weight if i ==1 else 1 for i in Y_train])\n",
        "        #test_sample_weight = np.array([weight if i ==1 else 1 for i in Y_test])\n",
        "\n",
        "        model_name = \"/content/drive/MyDrive/temp/bert_model/model_\" + str(fold)\n",
        "        #model = cnn(emb_dim, seq_len, num_filters, kernel_sizes, num_labels)\n",
        "        model = lstm_cnn(emb_dim, seq_len, 100, \\\n",
        "                         num_filters, kernel_sizes, num_labels)\n",
        "        model.to(device)\n",
        "\n",
        "\n",
        "        model, training_stats = train_single_label_model(model, num_labels, labels, train_dataloader, validation_dataloader, \\\n",
        "                                                         model_path = model_name, class_weight = class_weight,\n",
        "                                                        optimizer=None, scheduler=None, epochs = 20)\n",
        "        \n",
        "        print(\"load the best model ... \")\n",
        "\n",
        "        model.load_state_dict(torch.load(model_name))\n",
        "\n",
        "        # show performance of best model\n",
        "        model.eval()\n",
        "        tokenized_texts, pred_labels, true_labels,avg_val_loss = model_eval(model, validation_dataloader, class_weight = class_weight)\n",
        "\n",
        "        pred_bools = np.argmax(pred_labels, axis = 1)\n",
        "        true_bools = np.argmax(true_labels, axis = 1)\n",
        "\n",
        "        p, r, f, _ = precision_recall_fscore_support(true_bools,pred_bools, pos_label = 1)\n",
        "        #val_f1 = f1_score(true_bools,pred_bools, average = None)*100 \n",
        "        #val_f1 = val_f1[1] # return f1 for  class 1\n",
        "        val_acc = (pred_bools == true_bools).astype(int).sum()/len(pred_bools)\n",
        "   \n",
        "    \n",
        "        print('Precision: {0:.4f}, Recall: {1:.4f}, F1: {2:.4f}, Loss: {3:.4f}'.format(p[1], r[1], f[1], avg_val_loss))\n",
        "        print(classification_report(true_bools, pred_bools) )\n",
        "\n",
        "        \n",
        "    \n",
        "        #p, r, f = train_model(model, X_train, Y_train, train_sample_weight,\\\n",
        "        #                   X_test, Y_test, test_sample_weight, \\\n",
        "        #                   'baseline_models/lstm_cnn/'+col)\n",
        "\n",
        "        result.append([col, fold, p[1], r[1], f[1], training_stats[-1][\"Best epoch\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1oX2u74djlwQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cddacbed-530f-483a-ecda-372fc7052325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "   precision    recall        f1  epoch\n",
            "0   0.708333  0.629630  0.666667      4\n",
            "1   0.712644  0.756098  0.733728      2\n",
            "2   0.758621  0.804878  0.781065      8\n",
            " \n",
            "       precision    recall        f1     epoch\n",
            "label                                         \n",
            "label   0.726533  0.730202  0.727153  4.666667\n"
          ]
        }
      ],
      "source": [
        "result_df = pd.DataFrame(result, columns =[\"label\",\"fold\",\"precision\",\"recall\",\"f1\",\"epoch\"])\n",
        "\n",
        "for col in label_cols:\n",
        "    print(col)\n",
        "    print(result_df[result_df.label == col][[\"precision\",\"recall\",\"f1\",\"epoch\"]])\n",
        "    print(\" \")\n",
        "print(result_df[[\"label\",\"precision\",\"recall\",\"f1\",\"epoch\"]].groupby(\"label\").mean())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "loss = [[i, item['Training Loss'], item['Valid. Loss']] for i, item in enumerate(training_stats)]\n",
        "acc = [[item[\"Best epoch\"], 'Valid. Accur.'] for item in training_stats]\n",
        "\n",
        "pd.DataFrame(loss, columns=[\"epoch\", \"train_loss\",\"val_loss\"]).set_index(\"epoch\").plot(kind=\"line\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j99LBCGGHaZw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "0a8113f0-baba-435d-927b-de520a390c63"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd8b91d9ac0>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL_NnDGxRpEI"
      },
      "source": [
        "# 7. Train a model with all data for prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9CMlS_dkAGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36859758-eab6-4fed-a24b-2dfe6553e53a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-00ff6d653c96>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_train =  torch.tensor(vectors)\n",
            "<ipython-input-38-00ff6d653c96>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_val =  torch.tensor(vectors[0:100])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lstm_cnn(\n",
              "  (lstm): LSTM(768, 100, batch_first=True, bidirectional=True)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 64, kernel_size=(1, 200), stride=(1, 1))\n",
              "    (1): Conv2d(1, 64, kernel_size=(3, 200), stride=(1, 1))\n",
              "    (2): Conv2d(1, 64, kernel_size=(5, 200), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=192, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\t Train Loss: 0.6567\t Val Loss 0.5651\t Val Acc: 0.8100\t Val F1: 88.0503\n",
            "model saved\n",
            "Epoch 2\t Train Loss: 0.5071\t Val Loss 0.4252\t Val Acc: 0.8600\t Val F1: 91.1392\n",
            "model saved\n",
            "Epoch 3\t Train Loss: 0.3702\t Val Loss 0.1683\t Val Acc: 0.9300\t Val F1: 95.9064\n",
            "model saved\n",
            "Epoch 4\t Train Loss: 0.2547\t Val Loss 0.0982\t Val Acc: 0.9700\t Val F1: 98.2249\n",
            "model saved\n",
            "Epoch 5\t Train Loss: 0.1427\t Val Loss 0.0568\t Val Acc: 0.9900\t Val F1: 99.4083\n",
            "model saved\n",
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "emb_dim = vectors.size(-1)\n",
        "seq_len = vectors.size(1)\n",
        "num_filters = 64\n",
        "kernel_sizes = [1, 3, 5]\n",
        "num_labels = 2\n",
        "labels = ['0','1']\n",
        "class_weight = [1.0,1.0]\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "result = []\n",
        "X_train =  torch.tensor(vectors)\n",
        "Y_train = torch.tensor(pd.get_dummies(df.label).values)\n",
        "\n",
        "X_val =  torch.tensor(vectors[0:100])\n",
        "Y_val = torch.tensor(pd.get_dummies(df.label).values[0:100])\n",
        "\n",
        "# not needed. just to fulfill the traning function need\n",
        "train_dataset = TensorDataset(X_train, Y_train)\n",
        "val_dataset = TensorDataset(X_val, Y_val)  \n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,  # The training samples.\n",
        "    sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "    batch_size = batch_size # Trains with this batch size.\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "    val_dataset, # The validation samples.\n",
        "    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "    batch_size = batch_size # Evaluate with this batch size.\n",
        ")\n",
        "\n",
        "model_name = '/content/drive/MyDrive/temp/bert_model/genbert_forpred_v2'\n",
        "model = lstm_cnn(emb_dim, seq_len, 100, num_filters, kernel_sizes, num_labels)\n",
        "\n",
        "model.to(device)\n",
        "model, training_stats = train_single_label_model(model, num_labels, labels, train_dataloader, validation_dataloader, \\\n",
        "                                                         model_path = model_name, class_weight = class_weight,\n",
        "                                                        optimizer=None, scheduler=None, \\\n",
        "                                                        epochs = epochs, patience = 8)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNMfeMGJ9ipY"
      },
      "source": [
        "# Predict sentences"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0hEii1FGP88n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OsAkD8KSPW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "763aa6da-f767-493b-879a-5e2174ff39d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "bert_pred = pd.read_csv('/content/drive/MyDrive/temp/raw_text_forbert.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "DD120gtXSPhu",
        "outputId": "dc3d0879-bcff-4e99-ee91-b979e929a59d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text encoded_unique_ticker_ts  \\\n",
              "0  Chun Hong, you're accusing DRAM of being high ...                   new386   \n",
              "1  So you are talking about trending issue rather...                   new386   \n",
              "2  What do you predict the -- I assume it will be...                   new386   \n",
              "3                                   [indiscernible].                   new386   \n",
              "4                           That covers the cooling?                   new386   \n",
              "\n",
              "   rid  \n",
              "0    1  \n",
              "1    2  \n",
              "2    3  \n",
              "3    4  \n",
              "4    5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-44b6ef78-4be7-45a1-a46d-3a94bb81a1a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>encoded_unique_ticker_ts</th>\n",
              "      <th>rid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Chun Hong, you're accusing DRAM of being high ...</td>\n",
              "      <td>new386</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So you are talking about trending issue rather...</td>\n",
              "      <td>new386</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What do you predict the -- I assume it will be...</td>\n",
              "      <td>new386</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[indiscernible].</td>\n",
              "      <td>new386</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>That covers the cooling?</td>\n",
              "      <td>new386</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44b6ef78-4be7-45a1-a46d-3a94bb81a1a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-44b6ef78-4be7-45a1-a46d-3a94bb81a1a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-44b6ef78-4be7-45a1-a46d-3a94bb81a1a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "bert_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_pred.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV3NSd1vYOTL",
        "outputId": "198c4c0a-f596-40db-ebb1-69b1725ea26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text                        30\n",
              "encoded_unique_ticker_ts     0\n",
              "rid                          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_pred = bert_pred.dropna()"
      ],
      "metadata": {
        "id": "3_TakBdoYS4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_pred.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyEiB-RHi-pN",
        "outputId": "f137f533-9d66-4029-ce6c-bda6896712e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4334161, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "1NEW0bWBAahp",
        "outputId": "5a30d3a3-0215-4139-8a71-5e8121f348a9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f29eb3ded9e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# class_weight = [1.0, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mthe_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mthe_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mthe_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthe_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lstm_cnn' is not defined"
          ]
        }
      ],
      "source": [
        "model_path= '/content/drive/MyDrive/temp/bert_model/genbert_forpred_v2'\n",
        "emb_dim = 768\n",
        "seq_len = 100\n",
        "num_filters = 64\n",
        "kernel_sizes = [1,3,5]\n",
        "num_labels = 2\n",
        "# labels = ['0','1']\n",
        "# class_weight = [1.0, 1] \n",
        "    \n",
        "the_model = lstm_cnn(emb_dim, seq_len, 100, num_filters, kernel_sizes, num_labels)\n",
        "the_model.load_state_dict(torch.load(model_path))\n",
        "the_model = the_model.to(device)\n",
        "the_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf_ids = bert_pred[\"encoded_unique_ticker_ts\"].unique().tolist()\n",
        "len(conf_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsBfoOzUYrcT",
        "outputId": "1ecccdbe-55f2-4358-e587-f194f4254d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59086"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_file = '/content/drive/MyDrive/temp/predict_skp_gen_v3.csv' # manully change to v2\n",
        "checkpoint = 0\n",
        "\n",
        "if os.path.isfile(target_file):\n",
        "  result = pd.read_csv(target_file)\n",
        "\n",
        "  if len(bert_pred) >0:\n",
        "    checkpoint = conf_ids.index(result[\"rid\"].iloc[-1])\n",
        "    checkpoint += 1\n",
        "  else:\n",
        "    result = pd.DataFrame([], columns = [\"encoded_unique_ticker_ts\",\"rid\", \"text\", \"predict\"])\n",
        "    result.to_csv(target_file, header=True, index = False)\n",
        "else:\n",
        "  result = pd.DataFrame([], columns = [\"encoded_unique_ticker_ts\",\"rid\", \"text\", \"predict\"])\n",
        "  result.to_csv(target_file, header=True, index = False)\n",
        "\n",
        "print(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmDte7QlYneS",
        "outputId": "7b5a101b-fd6b-4296-fd0f-ca3f07bf58b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoKkAiqbdVnQ",
        "outputId": "2af4af9f-ab18-4583-e4b5-f8473329b9c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "batch_size  = 200\n",
        "\n",
        "for cid in conf_ids[checkpoint:]:\n",
        "  result = bert_pred[bert_pred.encoded_unique_ticker_ts==cid].copy()\n",
        "  preds = []\n",
        "  for i in range(0, len(result), batch_size):\n",
        "    # get embedding\n",
        "    x, masks = get_pretrained_wordvector(result[\"text\"].iloc[i:(i+batch_size)], tokenizer, bert_model)\n",
        "    x =  x * (masks.unsqueeze(-1).to(device))  ##  broadcasting\n",
        "    x = x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      pred = the_model(x)\n",
        "      pred = torch.softmax(pred, dim = -1)\n",
        "      pred = pred[:,-1].detach().cpu().numpy()\n",
        "\n",
        "      preds.append(pred)\n",
        "     \n",
        "  result[\"predict\"] = np.concatenate(preds, axis = 0)\n",
        "  result.to_csv(target_file, header=False, index= False, mode='a')\n",
        "\n",
        "  checkpoint += 1\n",
        "\n",
        "  if checkpoint%100 ==0:\n",
        "    print(\"{0}: {1: .2f}\".format(checkpoint, time.time()-start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi722JC2YngX",
        "outputId": "c5b96d5a-f91b-4cd6-de33-69cacc283e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:  60.09\n",
            "200:  129.33\n",
            "300:  193.68\n",
            "400:  263.03\n",
            "500:  336.56\n",
            "600:  407.21\n",
            "700:  467.17\n",
            "800:  532.67\n",
            "900:  605.18\n",
            "1000:  679.97\n",
            "1100:  751.46\n",
            "1200:  821.26\n",
            "1300:  898.78\n",
            "1400:  967.94\n",
            "1500:  1034.39\n",
            "1600:  1094.63\n",
            "1700:  1164.98\n",
            "1800:  1233.08\n",
            "1900:  1299.02\n",
            "2000:  1372.49\n",
            "2100:  1448.78\n",
            "2200:  1510.35\n",
            "2300:  1571.87\n",
            "2400:  1644.30\n",
            "2500:  1721.94\n",
            "2600:  1793.39\n",
            "2700:  1857.26\n",
            "2800:  1914.84\n",
            "2900:  1985.78\n",
            "3000:  2051.05\n",
            "3100:  2127.33\n",
            "3200:  2194.85\n",
            "3300:  2268.47\n",
            "3400:  2348.14\n",
            "3500:  2423.26\n",
            "3600:  2504.52\n",
            "3700:  2582.12\n",
            "3800:  2661.56\n",
            "3900:  2739.55\n",
            "4000:  2791.25\n",
            "4100:  2858.62\n",
            "4200:  2933.23\n",
            "4300:  2993.49\n",
            "4400:  3077.76\n",
            "4500:  3143.23\n",
            "4600:  3231.72\n",
            "4700:  3343.22\n",
            "4800:  3423.16\n",
            "4900:  3494.86\n",
            "5000:  3583.45\n",
            "5100:  3654.80\n",
            "5200:  3711.31\n",
            "5300:  3782.93\n",
            "5400:  3871.86\n",
            "5500:  3928.04\n",
            "5600:  3998.78\n",
            "5700:  4086.70\n",
            "5800:  4159.16\n",
            "5900:  4239.75\n",
            "6000:  4293.87\n",
            "6100:  4351.97\n",
            "6200:  4440.19\n",
            "6300:  4506.91\n",
            "6400:  4575.11\n",
            "6500:  4652.71\n",
            "6600:  4751.79\n",
            "6700:  4816.54\n",
            "6800:  4883.42\n",
            "6900:  4949.30\n",
            "7000:  5040.86\n",
            "7100:  5116.22\n",
            "7200:  5187.18\n",
            "7300:  5271.08\n",
            "7400:  5342.77\n",
            "7500:  5419.56\n",
            "7600:  5484.70\n",
            "7700:  5553.82\n",
            "7800:  5641.97\n",
            "7900:  5720.72\n",
            "8000:  5780.91\n",
            "8100:  5856.44\n",
            "8200:  5938.08\n",
            "8300:  6005.65\n",
            "8400:  6100.80\n",
            "8500:  6169.17\n",
            "8600:  6260.74\n",
            "8700:  6344.60\n",
            "8800:  6429.27\n",
            "8900:  6502.72\n",
            "9000:  6573.90\n",
            "9100:  6644.26\n",
            "9200:  6731.01\n",
            "9300:  6819.36\n",
            "9400:  6888.72\n",
            "9500:  6964.42\n",
            "9600:  7036.00\n",
            "9700:  7092.30\n",
            "9800:  7176.86\n",
            "9900:  7258.93\n",
            "10000:  7328.15\n",
            "10100:  7415.72\n",
            "10200:  7485.18\n",
            "10300:  7547.85\n",
            "10400:  7601.19\n",
            "10500:  7680.23\n",
            "10600:  7741.16\n",
            "10700:  7794.91\n",
            "10800:  7857.13\n",
            "10900:  7925.80\n",
            "11000:  8005.19\n",
            "11100:  8096.11\n",
            "11200:  8163.78\n",
            "11300:  8226.78\n",
            "11400:  8305.30\n",
            "11500:  8367.15\n",
            "11600:  8440.49\n",
            "11700:  8502.54\n",
            "11800:  8571.13\n",
            "11900:  8643.04\n",
            "12000:  8720.89\n",
            "12100:  8794.26\n",
            "12200:  8861.51\n",
            "12300:  8932.16\n",
            "12400:  9000.62\n",
            "12500:  9075.87\n",
            "12600:  9154.25\n",
            "12700:  9222.11\n",
            "12800:  9293.84\n",
            "12900:  9368.07\n",
            "13000:  9438.12\n",
            "13100:  9518.37\n",
            "13200:  9590.37\n",
            "13300:  9662.59\n",
            "13400:  9724.09\n",
            "13500:  9800.85\n",
            "13600:  9879.94\n",
            "13700:  9945.70\n",
            "13800:  10029.16\n",
            "13900:  10114.41\n",
            "14000:  10209.84\n",
            "14100:  10265.55\n",
            "14200:  10342.49\n",
            "14300:  10415.70\n",
            "14400:  10500.41\n",
            "14500:  10612.01\n",
            "14600:  10685.75\n",
            "14700:  10770.24\n",
            "14800:  10840.89\n",
            "14900:  10936.36\n",
            "15000:  11021.44\n",
            "15100:  11091.62\n",
            "15200:  11164.57\n",
            "15300:  11249.96\n",
            "15400:  11317.94\n",
            "15500:  11394.56\n",
            "15600:  11455.43\n",
            "15700:  11524.80\n",
            "15800:  11607.31\n",
            "15900:  11677.96\n",
            "16000:  11758.34\n",
            "16100:  11830.03\n",
            "16200:  11931.93\n",
            "16300:  12001.53\n",
            "16400:  12087.46\n",
            "16500:  12146.92\n",
            "16600:  12202.29\n",
            "16700:  12270.89\n",
            "16800:  12333.04\n",
            "16900:  12411.28\n",
            "17000:  12483.44\n",
            "17100:  12553.55\n",
            "17200:  12619.29\n",
            "17300:  12675.88\n",
            "17400:  12748.54\n",
            "17500:  12822.29\n",
            "17600:  12898.70\n",
            "17700:  12962.27\n",
            "17800:  13045.93\n",
            "17900:  13115.90\n",
            "18000:  13191.97\n",
            "18100:  13270.45\n",
            "18200:  13334.41\n",
            "18300:  13405.37\n",
            "18400:  13470.07\n",
            "18500:  13555.67\n",
            "18600:  13625.68\n",
            "18700:  13697.22\n",
            "18800:  13771.23\n",
            "18900:  13833.83\n",
            "19000:  13913.11\n",
            "19100:  13985.98\n",
            "19200:  14050.27\n",
            "19300:  14117.58\n",
            "19400:  14175.52\n",
            "19500:  14255.22\n",
            "19600:  14326.64\n",
            "19700:  14386.59\n",
            "19800:  14483.72\n",
            "19900:  14578.84\n",
            "20000:  14646.45\n",
            "20100:  14723.81\n",
            "20200:  14789.00\n",
            "20300:  14861.71\n",
            "20400:  14918.47\n",
            "20500:  14993.32\n",
            "20600:  15074.48\n",
            "20700:  15135.14\n",
            "20800:  15206.15\n",
            "20900:  15278.55\n",
            "21000:  15328.87\n",
            "21100:  15392.44\n",
            "21200:  15459.97\n",
            "21300:  15529.39\n",
            "21400:  15590.17\n",
            "21500:  15652.96\n",
            "21600:  15721.27\n",
            "21700:  15789.08\n",
            "21800:  15844.53\n",
            "21900:  15910.63\n",
            "22000:  15971.40\n",
            "22100:  16042.51\n",
            "22200:  16112.23\n",
            "22300:  16177.29\n",
            "22400:  16246.98\n",
            "22500:  16327.72\n",
            "22600:  16404.08\n",
            "22700:  16476.44\n",
            "22800:  16533.74\n",
            "22900:  16599.30\n",
            "23000:  16670.95\n",
            "23100:  16744.58\n",
            "23200:  16825.31\n",
            "23300:  16893.71\n",
            "23400:  16972.69\n",
            "23500:  17035.76\n",
            "23600:  17094.24\n",
            "23700:  17162.99\n",
            "23800:  17227.19\n",
            "23900:  17298.24\n",
            "24000:  17371.53\n",
            "24100:  17449.74\n",
            "24200:  17521.53\n",
            "24300:  17593.86\n",
            "24400:  17665.38\n",
            "24500:  17737.04\n",
            "24600:  17823.11\n",
            "24700:  17876.42\n",
            "24800:  17945.12\n",
            "24900:  18014.47\n",
            "25000:  18094.18\n",
            "25100:  18168.72\n",
            "25200:  18240.34\n",
            "25300:  18313.12\n",
            "25400:  18388.49\n",
            "25500:  18444.06\n",
            "25600:  18508.91\n",
            "25700:  18583.69\n",
            "25800:  18663.01\n",
            "25900:  18729.68\n",
            "26000:  18838.49\n",
            "26100:  18912.22\n",
            "26200:  18979.96\n",
            "26300:  19068.93\n",
            "26400:  19152.22\n",
            "26500:  19234.20\n",
            "26600:  19304.62\n",
            "26700:  19380.23\n",
            "26800:  19440.44\n",
            "26900:  19502.50\n",
            "27000:  19568.21\n",
            "27100:  19636.16\n",
            "27200:  19704.78\n",
            "27300:  19769.56\n",
            "27400:  19832.71\n",
            "27500:  19925.36\n",
            "27600:  19988.74\n",
            "27700:  20052.36\n",
            "27800:  20118.27\n",
            "27900:  20194.62\n",
            "28000:  20272.25\n",
            "28100:  20341.22\n",
            "28200:  20419.59\n",
            "28300:  20488.24\n",
            "28400:  20554.05\n",
            "28500:  20628.21\n",
            "28600:  20711.64\n",
            "28700:  20780.59\n",
            "28800:  20851.00\n",
            "28900:  20935.07\n",
            "29000:  21009.05\n",
            "29100:  21076.45\n",
            "29200:  21139.71\n",
            "29300:  21214.34\n",
            "29400:  21286.07\n",
            "29500:  21358.62\n",
            "29600:  21443.64\n",
            "29700:  21527.12\n",
            "29800:  21592.64\n",
            "29900:  21660.15\n",
            "30000:  21735.43\n",
            "30100:  21810.02\n",
            "30200:  21873.04\n",
            "30300:  21967.40\n",
            "30400:  22046.22\n",
            "30500:  22116.42\n",
            "30600:  22192.49\n",
            "30700:  22269.58\n",
            "30800:  22337.28\n",
            "30900:  22409.54\n",
            "31000:  22471.13\n",
            "31100:  22547.55\n",
            "31200:  22621.60\n",
            "31300:  22697.18\n",
            "31400:  22760.42\n",
            "31500:  22825.72\n",
            "31600:  22901.89\n",
            "31700:  22980.42\n",
            "31800:  23043.58\n",
            "31900:  23113.11\n",
            "32000:  23188.01\n",
            "32100:  23250.72\n",
            "32200:  23310.23\n",
            "32300:  23407.93\n",
            "32400:  23479.40\n",
            "32500:  23560.43\n",
            "32600:  23628.24\n",
            "32700:  23702.73\n",
            "32800:  23759.42\n",
            "32900:  23830.53\n",
            "33000:  23931.04\n",
            "33100:  24006.06\n",
            "33200:  24091.12\n",
            "33300:  24159.83\n",
            "33400:  24228.20\n",
            "33500:  24302.05\n",
            "33600:  24365.79\n",
            "33700:  24431.22\n",
            "33800:  24494.85\n",
            "33900:  24576.93\n",
            "34000:  24638.29\n",
            "34100:  24709.61\n",
            "34200:  24778.13\n",
            "34300:  24833.20\n",
            "34400:  24901.91\n",
            "34500:  24976.44\n",
            "34600:  25046.34\n",
            "34700:  25131.95\n",
            "34800:  25221.28\n",
            "34900:  25285.93\n",
            "35000:  25365.68\n",
            "35100:  25448.30\n",
            "35200:  25507.64\n",
            "35300:  25567.70\n",
            "35400:  25652.09\n",
            "35500:  25730.85\n",
            "35600:  25804.73\n",
            "35700:  25875.62\n",
            "35800:  25937.32\n",
            "35900:  26018.02\n",
            "36000:  26097.67\n",
            "36100:  26178.48\n",
            "36200:  26241.35\n",
            "36300:  26312.81\n",
            "36400:  26387.68\n",
            "36500:  26456.64\n",
            "36600:  26538.11\n",
            "36700:  26617.80\n",
            "36800:  26684.99\n",
            "36900:  26740.05\n",
            "37000:  26809.96\n",
            "37100:  26865.86\n",
            "37200:  26926.20\n",
            "37300:  26988.54\n",
            "37400:  27063.10\n",
            "37500:  27137.87\n",
            "37600:  27227.90\n",
            "37700:  27295.21\n",
            "37800:  27356.85\n",
            "37900:  27426.63\n",
            "38000:  27497.82\n",
            "38100:  27565.76\n",
            "38200:  27634.61\n",
            "38300:  27691.78\n",
            "38400:  27748.99\n",
            "38500:  27813.97\n",
            "38600:  27911.03\n",
            "38700:  27988.90\n",
            "38800:  28057.65\n",
            "38900:  28113.94\n",
            "39000:  28183.09\n",
            "39100:  28265.41\n",
            "39200:  28339.96\n",
            "39300:  28420.46\n",
            "39400:  28492.68\n",
            "39500:  28573.83\n",
            "39600:  28647.12\n",
            "39700:  28731.08\n",
            "39800:  28807.75\n",
            "39900:  28874.85\n",
            "40000:  28940.01\n",
            "40100:  29022.60\n",
            "40200:  29084.38\n",
            "40300:  29171.06\n",
            "40400:  29239.70\n",
            "40500:  29313.08\n",
            "40600:  29392.97\n",
            "40700:  29462.37\n",
            "40800:  29539.23\n",
            "40900:  29599.63\n",
            "41000:  29661.43\n",
            "41100:  29736.64\n",
            "41200:  29810.41\n",
            "41300:  29878.91\n",
            "41400:  29970.08\n",
            "41500:  30038.59\n",
            "41600:  30109.41\n",
            "41700:  30193.92\n",
            "41800:  30289.81\n",
            "41900:  30367.39\n",
            "42000:  30434.71\n",
            "42100:  30510.66\n",
            "42200:  30586.23\n",
            "42300:  30651.59\n",
            "42400:  30716.34\n",
            "42500:  30789.94\n",
            "42600:  30863.29\n",
            "42700:  30925.08\n",
            "42800:  30991.73\n",
            "42900:  31073.55\n",
            "43000:  31139.62\n",
            "43100:  31209.57\n",
            "43200:  31275.87\n",
            "43300:  31356.67\n",
            "43400:  31430.17\n",
            "43500:  31516.57\n",
            "43600:  31597.60\n",
            "43700:  31659.66\n",
            "43800:  31746.69\n",
            "43900:  31810.22\n",
            "44000:  31873.05\n",
            "44100:  31939.16\n",
            "44200:  32003.62\n",
            "44300:  32077.70\n",
            "44400:  32153.52\n",
            "44500:  32226.62\n",
            "44600:  32295.24\n",
            "44700:  32372.43\n",
            "44800:  32445.61\n",
            "44900:  32509.88\n",
            "45000:  32561.07\n",
            "45100:  32619.37\n",
            "45200:  32703.12\n",
            "45300:  32785.69\n",
            "45400:  32855.02\n",
            "45500:  32928.12\n",
            "45600:  32993.65\n",
            "45700:  33067.78\n",
            "45800:  33127.57\n",
            "45900:  33182.47\n",
            "46000:  33258.61\n",
            "46100:  33318.28\n",
            "46200:  33386.09\n",
            "46300:  33459.77\n",
            "46400:  33513.51\n",
            "46500:  33589.40\n",
            "46600:  33663.91\n",
            "46700:  33739.88\n",
            "46800:  33812.99\n",
            "46900:  33875.94\n",
            "47000:  33946.72\n",
            "47100:  34005.88\n",
            "47200:  34072.29\n",
            "47300:  34134.51\n",
            "47400:  34200.42\n",
            "47500:  34284.45\n",
            "47600:  34362.66\n",
            "47700:  34424.75\n",
            "47800:  34492.06\n",
            "47900:  34556.96\n",
            "48000:  34631.51\n",
            "48100:  34702.63\n",
            "48200:  34767.15\n",
            "48300:  34845.03\n",
            "48400:  34911.70\n",
            "48500:  34977.22\n",
            "48600:  35036.28\n",
            "48700:  35104.72\n",
            "48800:  35176.96\n",
            "48900:  35243.59\n",
            "49000:  35302.83\n",
            "49100:  35363.79\n",
            "49200:  35433.67\n",
            "49300:  35492.35\n",
            "49400:  35560.00\n",
            "49500:  35627.84\n",
            "49600:  35699.85\n",
            "49700:  35768.25\n",
            "49800:  35838.32\n",
            "49900:  35918.65\n",
            "50000:  35983.23\n",
            "50100:  36047.31\n",
            "50200:  36129.11\n",
            "50300:  36209.69\n",
            "50400:  36271.26\n",
            "50500:  36339.03\n",
            "50600:  36419.77\n",
            "50700:  36483.23\n",
            "50800:  36557.85\n",
            "50900:  36611.91\n",
            "51000:  36664.99\n",
            "51100:  36728.71\n",
            "51200:  36801.51\n",
            "51300:  36876.07\n",
            "51400:  36936.80\n",
            "51500:  37018.87\n",
            "51600:  37076.52\n",
            "51700:  37141.69\n",
            "51800:  37216.78\n",
            "51900:  37287.06\n",
            "52000:  37350.64\n",
            "52100:  37420.63\n",
            "52200:  37489.34\n",
            "52300:  37565.00\n",
            "52400:  37637.65\n",
            "52500:  37696.45\n",
            "52600:  37760.04\n",
            "52700:  37814.22\n",
            "52800:  37889.57\n",
            "52900:  37955.50\n",
            "53000:  38033.29\n",
            "53100:  38106.18\n",
            "53200:  38185.20\n",
            "53300:  38262.72\n",
            "53400:  38326.61\n",
            "53500:  38404.12\n",
            "53600:  38476.01\n",
            "53700:  38546.30\n",
            "53800:  38605.34\n",
            "53900:  38672.18\n",
            "54000:  38752.38\n",
            "54100:  38821.36\n",
            "54200:  38894.57\n",
            "54300:  38968.20\n",
            "54400:  39046.21\n",
            "54500:  39112.35\n",
            "54600:  39186.29\n",
            "54700:  39259.68\n",
            "54800:  39314.78\n",
            "54900:  39393.49\n",
            "55000:  39469.29\n",
            "55100:  39536.88\n",
            "55200:  39593.30\n",
            "55300:  39654.89\n",
            "55400:  39717.17\n",
            "55500:  39782.87\n",
            "55600:  39853.19\n",
            "55700:  39937.39\n",
            "55800:  40004.25\n",
            "55900:  40086.79\n",
            "56000:  40154.61\n",
            "56100:  40219.23\n",
            "56200:  40282.87\n",
            "56300:  40351.80\n",
            "56400:  40416.05\n",
            "56500:  40487.98\n",
            "56600:  40559.78\n",
            "56700:  40631.26\n",
            "56800:  40698.37\n",
            "56900:  40769.33\n",
            "57000:  40844.89\n",
            "57100:  40922.79\n",
            "57200:  40995.36\n",
            "57300:  41070.50\n",
            "57400:  41137.93\n",
            "57500:  41214.25\n",
            "57600:  41297.16\n",
            "57700:  41354.89\n",
            "57800:  41410.83\n",
            "57900:  41469.26\n",
            "58000:  41531.24\n",
            "58100:  41606.91\n",
            "58200:  41665.56\n",
            "58300:  41730.47\n",
            "58400:  41810.70\n",
            "58500:  41898.55\n",
            "58600:  41958.72\n",
            "58700:  42024.04\n",
            "58800:  42096.62\n",
            "58900:  42167.03\n",
            "59000:  42227.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZXqK0r9YZH0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SS7Mm5gdYnlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MLrMP2avP8_U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}